---
title: "STAA 567 Assignment 3 - Optimization Methods"
author: "Jon Dollard"
date: "11/9/2020"
output: pdf_document
---

```{r setup, include=FALSE, fig.show='hold', out.width="50%"}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
library(gridExtra) #gives you grid.arrange()
library(knitr)     #making tables
```

The goal of this assignment is to learn more about optimization methods.  We'll use optimization methods to find the maximum likelihood estimate of a parameter.  In this problem we'll use data which are i.i.d. (independent and indentically distributed) sample from a $Cauchy(\theta,1)$ distribution:

```{r}
#Create a vector of the data from the Cauchy distribution
x <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 3.29, 3.71, -2.40,
       4.53, -0.07, -1.05, -13.87, -2.53, -1.75, 0.27, 43.21)
```

The Cauchy distribution is a heavy-tailed distribution.  It is characterized by generating infrequently large deviations from the its mean.  The probability density function of a $Cauchy(\theta,1)$ is given by

$f(x;\theta) = \frac{1}{\pi(1 + (x - \theta)^2)}$

Questions:

1. Write the likelihood $\cal L$ of an i.i.d. sample $x_1,.....,x_n$ from a $Cauchy(\theta,1)$ distribution.  Now derive the log-likelihood $\ell$, the first derivative $\ell'$, and the second derivative $\ell''$.

Likelihood:

$\cal L$$(\theta; x_1,...,x_n)$ = $\displaystyle \prod_{i=1}^{n} \frac{1}{\pi(1 + (x_i - \theta)^2)}$ = $\frac{1}{\pi^n}\displaystyle \prod_{i=1}^{n} \frac{1}{(1 + (x_i - \theta)^2)}$

log-likelihood:

$\ell$$(\theta; x_1,...,x_n)$ = $-n\log(\pi) - \displaystyle \sum_{i=1}^{n} log[1 + (x_i - \theta)^2]$

$\ell'$:

$\ell'$ = $2 \displaystyle \sum_{i=1}^{n} \frac{x_i - \theta}{1 + (x_i - \theta)^2}$

$\ell''$:

$\ell''$ = $2 \displaystyle \sum_{i = 1}^n\frac{(x_i - \theta)^2 - 1}{(1 + (x_i - \theta)^2)^2}$

2. Graph the log likelihood as a funtion of $\theta(-20 < \theta < 50)$ given the above data set.

```{r, warning=FALSE}
#set a range of values for theta
theta <- seq(-20, 50, 0.1)

#Create a function for the log likelihood
log_like_func <- function(theta, x){
  llikelihood <- c()  
  for (i in 1:length(theta)){
      llikelihood[i] <- (-length(x) * log (pi) - sum(log(1 + (x - theta[i])^2)))
    }
  return(llikelihood)
}

#Create a data frame for use with ggplot
log_like_1 <- log_like_func(theta, x)
like_df <- data.frame(log_like_1, theta)

#Use ggplot to make a nice plot of the log likelihood vs. theta
ggplot(data = like_df, aes(x = theta, y = log_like_1)) +
         geom_line(color = "purple", lwd = 1.5) +
         labs(x = expression(theta), y = "log likelihood", 
         title = expression(paste("log likelihood of a Cauchy (",theta, ",1) Distribution  vs. ",theta))) +               
         theme(plot.title = element_text(hjust = 0.5))
```

3. Find the maximum likelihood estimator for $\theta$ (i.e., maximize the logarithm of the likelihood) using your Newton-Raphson algorithm from the previous assignment.  Try all of the following starting points:

```{r}
x_mean <- mean(x)
start_values <- c(-11, -1, 0, 1.5, 4, 4.7, x_mean, 7, 8, 38)

#Newton-Raphson method from assignment 2
newtons_meth <- function(h, x, theta_0, iter, eps){
	#h is the ratio of g'/g''
  #x is the data
  #theta_0 is the initial guess for theta
  #iter is the number of iterations
  #eps is the error term and stop condition
  
  theta_opt_newt <<- NULL         #Initialize the theta_opt_newt variable
                                  #(global variable for use outside the function)
  i_newt <<- NULL                 #Initialize the loop variable 
                                  #(global variable for use outside the function)
  
  theta_opt_newt <- theta_0
	
  for (i_newt in 1:iter){
		 
       #Code to implement the Newton-Raphson method
        theta_opt_newt[i_newt + 1] <- theta_opt_newt[i_newt] - h(theta_opt_newt[i_newt], x)        
		
          #Code to check for convergence criteria
          if(i_newt > 1 && (abs(theta_opt_newt[i_newt + 1] - theta_opt_newt[i_newt]) < eps)) {
	    
	          #accuracy criteria met, output a message to the user that includes the number of
            #iterations and the value of theta
	          cat("The number of iterations for convergence is", i_newt,".",
	          "The optimized value of the function is", theta_opt_newt[i_newt + 1])
	      
	          #set final values for global variables 
            theta_opt_newt <<- theta_opt_newt[i_newt + 1]
            i_newt <<- i_newt
       
            #silent return so that NULL isn't output
            return(cat(""))
	      
          }
        
       }
    
    #Code to check for lack of convergence
    if(abs(theta_opt_newt[i_newt + 1] - theta_opt_newt[i_newt]) >= eps){
    
    #Warning message to the user if Newton's method fails to converge
    cat("Warning - This method failed to converge in", iter, "iterations.")
             #Set final values for global variable
             theta_opt_newt <<- theta_opt_newt[i_newt + 1]
             i_newt <<- NA
    }

}

#Create a function for the derivative of the log likelihood
dll <- function(theta, x){2*sum((x - theta) / (1 + (x - theta)^2))}

#Create a function for the second derivative of the log likelihood
d2ll <- function(theta, x){2*sum(((x - theta)^2 - 1) / ((1 + (x - theta)^2)^2))}

#Create a function for the ratio of dll and d2ll
dll_d2ll <- function(theta, x){(2*sum((x - theta) / (1 + (x - theta)^2))) / 
                              (2*sum(((x - theta)^2 - 1) / ((1 + (x - theta)^2)^2)))}

eps = .01
iter = 100

#Create a vector to store all of the theta MLEs from the different start values
theta_MLE <- c(rep(NA, 10))
#Create a vector to store the number of iterations for the different start values
iter_MLE <- c(rep(NA, 10))

for(i in 1:length(start_values)){
      capture.output(newtons_meth(dll_d2ll, x, start_values[i],  iter, eps))
      theta_MLE[i] <- theta_opt_newt
      iter_MLE[i] <- i_newt
}

theta_MLE_matrix <- matrix(c(start_values, theta_MLE, iter_MLE), nrow = 10, ncol = 3)
theta_MLE_matrix
```
 \pagebreak
Fill in a nice looking table of the results:

```{r}
#Use kable to make a nice table of the Newton-Raphson results
kable(theta_MLE_matrix, col.names = c("Initial Value", "Est $\\Theta$", "Iterations"),
      align = "ccc", caption = 
        "Results of The Newton-Raphson Method at Different Initial Values")
```

Discuss your results.

The estimate of $\theta$ is dependent upon the initial value used in the Newton-Raphson method.  For the values of -11 and 8 the Newton-Raphson method fails to converge to any local optima (denoted as NA in the table above).  The method is fairly quick to find local optima when it can with the largest number of iterations required being 8.  For what appears to be the true maximum we see convergence in 2 or 3 iterations to a value of -0.192.  However, we also note that it is easy to fool the algorithm.  Starting with a value of 1.5 we find a different "peak" on the distribution at $\hat{\theta}$ = 1.714.  While quick in terms of number of iterations required to converge to a maximum value, the Newton-Raphson method does not appear to be very robust for use estimating the MLE of $\theta$ for the $Cauchy(\theta, 1)$ distribution.  

4. Find the maximum likelihood estimator for $\theta$ using the interval bisection method with starting points -1 and 1 $\epsilon$ = 0.01 (absolute stopping criteria).

```{r}
#Using the interval bisection method code from assignment 2 with slight mods

#initialize variables to be used in the function
l = -1
r = 1
iter = 100

int_bisec <- function(dg, x, l, r, iter, eps){
	#dg is the objective function
  #x is the data
  #l is the left endpoint of the interval
  #r is the right endpoint of the interval
  #iter is the number of iterations
  #eps is the error term and stop condition
  
  theta_opt_bisec <<- NULL    #Initialize the theta_opt_bisec variable
                              #(global variable for use outside the function)
  i_bisec <<- NULL            #Initialize the loop variable 
                              #(global variable for use outside the function)
  
  for (i_bisec in 1:iter){
      
        #Code to implement the bisection method
        theta_opt_bisec[i_bisec] <- (l+r)/2        
		
		    if (dg(l, x)*dg(theta_opt_bisec[i_bisec], x)<0){  
			
		      r <- theta_opt_bisec[i_bisec]
		
		    }
		
		    else {
		  
			    l <- theta_opt_bisec[i_bisec]
			
		    }
		
	      #Code to check for convergence criteria
        if(i_bisec > 1 && (abs(theta_opt_bisec[i_bisec] - theta_opt_bisec[i_bisec-1]) < eps)){
	    
	          #accuracy criteria met, output a message to the user that includes
            #the number of iterations and the value of theta
	          cat("The number of iterations for convergence is", i_bisec,".", 
	          "The optimized value of the function is", theta_opt_bisec[i_bisec])
	      
	          #set final values for global variables 
            theta_opt_bisec <<- theta_opt_bisec
            i_bisec <<- i_bisec
       
            #silent return so that NULL isn't output
            return(cat(""))
	      
	        }
	}
    
    #Code to check for lack of convergence
    if(abs(theta_opt_bisec[i_bisec] - theta_opt_bisec[i_bisec-1]) >= eps){
    
    #Warning message to the user if the bisection method fails to converge
    cat("Warning - This method failed to converge in", iter, "iterations.")
    }

}

#Run int_bisec for the range specified
int_bisec(dll, x, l, r, iter, eps)
```

5. From starting values of $(\theta^{(0)}, \theta^{(1)})$ = (-2, -1), apply the secant method to estimate $\theta$ with $\epsilon$ = 0.01.  What happens when $(\theta^{(0)}, \theta^{(1)})$ = (-3, 3)?

```{r}
#Use code for the secant method from assignment 2
secant_meth <- function(dg, x, theta_0, theta_1, iter, eps){
	#dg is the derivative of the object function
  #x is the data
  #x_0 is an initial guess
  #x_1 is an initial guess
  #iter is the number of iterations
  #eps is the error term and stop condition
  
  theta_opt_secant <<- NULL     #Initialize the theta_opt_newt variable
                                #(global variable for use outside the function)
  i_secant <<- NULL             #Initialize the loop variable
                                #(global variable for use outside the function)
  
  theta_opt_secant <- c()
  theta_opt_secant[1] <- theta_0
  theta_opt_secant[2] <- theta_1
	
  for (i_secant in 3:(iter + 3)){
		
       #Code to implement the secant method
       h <- dg(theta_opt_secant[i_secant - 1], x) * (theta_opt_secant[i_secant - 1] - 
                                                       theta_opt_secant[i_secant - 2]) / 
         ((dg(theta_opt_secant[i_secant - 1], x) - dg(theta_opt_secant[i_secant - 2], x)))
		  
       theta_opt_secant[i_secant] <- theta_opt_secant[i_secant - 1] - h        
		
          #Code to check for convergence criteria
          if(i_secant > 3 && (abs(theta_opt_secant[i_secant] - theta_opt_secant[i_secant - 1]) < eps)) {
	    
	          #accuracy criteria met, output a message to the user
            #that includes the number of iterations and the value of theta
	          cat("The number of iterations for convergence is", i_secant - 2,".",
	          "The optimized value of the function is", theta_opt_secant[i_secant])
	      
	          #set final values for global variables 
            theta_opt_secant <<- theta_opt_secant[i_secant]
            i_secant <<- i_secant - 2                  #Subtract 2 for our first 2 initial guesses
       
            #silent return so that NULL isn't output
            return(cat(""))
	      
          }
        
       }
    
    #Code to check for lack of convergence
    if(abs(theta_opt_secant[i_secant] - theta_opt_secant[i_secant - 1]) >= eps){
    
    #Warning message to the user if the secant method fails to converge
    cat("Warning - This method failed to converge in", iter, "iterations.")
    }

}

#initialize variables to be used in the function
iter = 100
theta_0 = -2
theta_1 = -1
secant_meth(dll, x, theta_0, theta_1, iter, eps)

#Run secant_meth again with different initial values
iter = 100
theta_0 = -3
theta_1 = 3
secant_meth(dll, x, theta_0, theta_1, iter, eps)
```

Very similar to what we observed with the Newton-Raphson method, the maximum value that the secant method converges on is dependent on the initial values we provide.  We see that for the first set of initial values we converge to the actual maximum value of $\hat{\theta}$.  However, when we change to the second set of initial values we see that we fool the algorithm and it converges to an incorrect value.  This behavior isn't surprising since the secant method is a variation of the Newton-Raphson method where instead of calculating the second derivative analytically we approximate it using the first derivative.  It is also rather quick to converge and just slightly slower than the Newton-Raphson method based on the initial values tested.

6. Find the maximum likelihood estimator for $\theta$ using the R function optimize with a starting interval [-1,1] and $\epsilon$ = 0.01.

```{r}
#Using modified code from assignment 2

#Set the interval
interval <- c(-1:1)

#Set the tolerance for convergence
eps <- 0.01

fun_opt <- function(g, x, interval, eps){
  opt <- optimize(g, interval, maximum = TRUE, tol = eps, x = x)
  return(opt)
}

opt_fun_out <- fun_opt(log_like_func, x, interval, eps)
opt_fun_out
```

7. Write a summary comparing the speed and stability of the Newton-Raphson method, interval bisection, secant method, and the optimize function.

Similar to what we have seen in previous work it appears that the Newton-Raphson method is the fastest method to converge although we note that we do not get the number of iterations required to converge from the optimize function.  The Newton-Raphson and secant method both appear to be less stable than the interval bisection method and the optimize function.  This may not be the right analogy to use but I see these optimization methods as very similar in nature to the bias-variance tradeoff that exists in statistical learning methods.  In this case the analogy is a speed vs. stability tradeoff.  It seems that the faster methods are less stable, especially when the function is not well behaved.  The slower methods seem much harder to fool, but take a factor of 2x iterations to converge.  In all cases it is helpful to graph the function to know what a good initial value/s or interval is.  This is fine for 2 and 3 dimensions, but for higher dimensional optimization problems we might be better off choosing a method that is more stable and recognize that our optimization will cost more computationally but result in a better estimate.  

Further exploration (optional): the problem here is that the Cauchy distribution makes for challenging estimation.  If you want to explore this issue further, apply the methods to a random sample of size 20 from a $N(\theta,1)$ distribution? Do your conclusions change?






























